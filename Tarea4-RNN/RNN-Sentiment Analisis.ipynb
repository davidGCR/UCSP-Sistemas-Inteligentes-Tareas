{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el presente trabajo se realiza Sentiment Analisis de revisiones de peliculas con redes neuronales recurrentes (RNN) multicapa, unidades LSMT unidireccionales y bidireccionales. Ademas se utilizan Wordembeddings preentrenados GloVe para la representacion numerica de texto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta etapa se realizan las siguientes tareas:\n",
    "- Carga del dataset.\n",
    "- Definicion de longitud maxima de una revision.\n",
    "- Definicion de dimension de wordembeddings.\n",
    "- Tokenizacion de cada \"movie review\".\n",
    "- Definicion del vocabulario del dataset.\n",
    "- Representacion de cada movie review como vectores de indices en el vocabulario del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Carga del dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.review\n",
    "y = df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Definicion de dimension de embeddings y longitut maxima de review\n",
    "\n",
    "import tensorflow as tf\n",
    "EMBEDDING_DIMENSION = 50 #dimension de embbedings\n",
    "MAX_REVIEW_LENGTH = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizacion de reviews\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NB_WORDS = 5000 # only more frequently used words will be kept    \n",
    "tokenizer = Tokenizer(num_words=MAX_REVIEW_LENGTH,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,split=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124252 palabras unicas.\n"
     ]
    }
   ],
   "source": [
    "# Definicion del vocabulario del dataset\n",
    "\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "word_index = tokenizer.word_index #vocabulario del dataset\n",
    "N_WORDS = len(word_index) #numero de palabras unicas en el dataset\n",
    "print('%s palabras unicas.' %N_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representacion de reviews como vectores de indices del vocabulario\n",
    "\n",
    "reviews_vectors = pad_sequences(sequences, maxlen=MAX_REVIEW_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 75,   3,   3, 194,  55,  18,  60,   6, 146,   4,   3,  16, 115,\n",
       "        10,   1, 108, 136,   3,  36,   1,  39,   5,  82,   9,   2,  44,\n",
       "        62,  34,   8,   9,   1,  35,   8,   9, 159,   6,   5,  87,  18,\n",
       "        28, 149,  62,  84, 115,  10,  21,  52,  28,   1,  67,   2,   5,\n",
       "        20,   9,  51, 119,  53,  30,  33,   5,  26,  84,   4,   1,  18,\n",
       "        14, 198,  44,  62,  35,  94,   1,   3,  16,   1,   2,  33,   1,\n",
       "         5,   1,   1, 173,   4,   1,  19,  60,   6,  62,   5,  15,   1,\n",
       "        84,   9,  39,   4,  57,  51,  16,  57,  51,   8,   9,   8,   3,\n",
       "        51,   2,  79,  16,  51,  15,   1, 164,  35, 184,   5,  25,  53,\n",
       "        20,  24,  38,   1,   5,  77,   1,   4,   1,   8,   1, 127,   3,\n",
       "       169,   4,   1, 168,  12,   1,  90,   1,  28,  66,  16,  35,  13,\n",
       "        24,   5,  16,   5,  25,  87,  24,   1,   2,  24,   1,  19, 100,\n",
       "       105,   4,   2, 113,   2,  91,   1,   2,   1,  36,   5, 103, 104,\n",
       "        16,  54, 111,  12,   5,  26, 137,  37, 104,  20, 136,  33,  23,\n",
       "       161,  18,   3,   4,  85,   4,  47, 104,  67,  76,  50,  33,  89,\n",
       "        25, 193,   5,  38,  57], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_vectors[111]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Division de dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta etapa se procede a dividir el dataset para Validacion Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  (50000,)\n",
      "y :  (50000,)\n",
      "X_train:  (40000, 200)\n",
      "y_train:  (40000,)\n",
      "X_test:  (10000, 200)\n",
      "y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews_vectors, y, test_size=0.2)\n",
    "print('X : ',X.shape)\n",
    "print('y : ',y.shape)\n",
    "\n",
    "print('X_train: ',X_train.shape)\n",
    "print('y_train: ',y_train.shape)\n",
    "print('X_test: ',X_test.shape)\n",
    "print('y_test: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step = 2\n",
    "# batch_size = 5\n",
    "# offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "# x = X_train[offset:(offset + batch_size),:]\n",
    "# y = y_train[offset:(offset + batch_size)]\n",
    "        \n",
    "# x.shape, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga de Embeddings pre-Entrenados (Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulario glove size:  1193513\n"
     ]
    }
   ],
   "source": [
    "glove_file = 'glove.twitter.27B.' + str(EMBEDDING_DIMENSION) + 'd.txt'\n",
    "emb_dict = {}\n",
    "glove = open(glove_file)\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.array(values[1:], dtype=np.float32)\n",
    "#     print(vector.shape)\n",
    "    if vector.shape[0]== EMBEDDING_DIMENSION:\n",
    "        emb_dict[word] = vector\n",
    "glove.close()\n",
    "print('vocabulario glove size: ',len(emb_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193513, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = np.array([emb_dict[i] for i in emb_dict.keys()])\n",
    "for i in range(embeddings.shape[0]):\n",
    "    embeddings[i] = embeddings[i].reshape(1,50)\n",
    "embeddings[0].shape\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion que devuelve dataset por minilotes\n",
    "def get_batches(x, y, batch_size=1000):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Definicion del Modelo: Construccion del grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta etapa se procede a construir el modelo RNN utilizando Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batchSize = 1000\n",
    "lstmUnits = 64\n",
    "numClasses = 1\n",
    "learning_rate = 0.01\n",
    "num_layers = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "labels = tf.placeholder(tf.int32,[batchSize,numClasses])\n",
    "    #ids\n",
    "inputs = tf.placeholder(tf.int32,[batchSize,MAX_REVIEW_LENGTH])\n",
    "data = tf.Variable(tf.zeros([batchSize, MAX_REVIEW_LENGTH, EMBEDDING_DIMENSION]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(embeddings,inputs)\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "#     y_pred = rnn_model(data,lstmUnits,numClasses)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte se definen las celdas LSMT y se le envuelve en una capa Dropout para regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsmt_cell():\n",
    "    lstm = tf.contrib.rnn.LSTMCell(lstmUnits, reuse=tf.get_variable_scope().reuse)\n",
    "    return tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([lsmt_cell() for _ in range(num_layers)])\n",
    "initial_state = cell.zero_state(batchSize, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion de Celdas LSTM Bidireccionales y capas\n",
    "\n",
    " \n",
    "# cell_fw = tf.nn.rnn_cell.LSTMCell(lstmUnits, reuse=tf.get_variable_scope().reuse)\n",
    "multi_cell_fw = tf.contrib.rnn.MultiRNNCell([lsmt_cell() for _ in range(num_layers)])\n",
    "# cell_bw = tf.nn.rnn_cell.LSTMCell(lstmUnits, reuse=tf.get_variable_scope().reuse)\n",
    "multi_cell_bw = tf.contrib.rnn.MultiRNNCell([lsmt_cell() for _ in range(num_layers)])\n",
    "initial_state = cell.zero_state(batchSize, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, final_state  = tf.nn.bidirectional_dynamic_rnn(multi_cell_fw, multi_cell_bw, data, initial_state_fw=initial_state,initial_state_bw=initial_state, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.concat(outputs,axis=2)\n",
    "predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "cost = tf.losses.mean_squared_error(labels, predictions)    \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion se procede a entrenar el modelo construido. El entrenamiento se realiza por minilotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 8 Train loss: 0.252\n",
      "Epoch: 0/10 Iteration: 16 Train loss: 0.251\n",
      "Epoch: 0/10 Iteration: 24 Train loss: 0.248\n",
      "Epoch: 0/10 Iteration: 32 Train loss: 0.234\n",
      "Epoch: 0/10 Iteration: 40 Train loss: 0.246\n",
      "Epoch: 1/10 Iteration: 48 Train loss: 0.241\n",
      "Val acc: 0.596\n",
      "Epoch: 1/10 Iteration: 56 Train loss: 0.235\n",
      "Epoch: 1/10 Iteration: 64 Train loss: 0.225\n",
      "Epoch: 1/10 Iteration: 72 Train loss: 0.224\n",
      "Epoch: 1/10 Iteration: 80 Train loss: 0.217\n",
      "Epoch: 2/10 Iteration: 88 Train loss: 0.209\n",
      "Epoch: 2/10 Iteration: 96 Train loss: 0.215\n",
      "Val acc: 0.684\n",
      "Epoch: 2/10 Iteration: 104 Train loss: 0.206\n",
      "Epoch: 2/10 Iteration: 112 Train loss: 0.194\n",
      "Epoch: 2/10 Iteration: 120 Train loss: 0.192\n",
      "Epoch: 3/10 Iteration: 128 Train loss: 0.205\n",
      "Epoch: 3/10 Iteration: 136 Train loss: 0.210\n",
      "Epoch: 3/10 Iteration: 144 Train loss: 0.194\n",
      "Val acc: 0.727\n",
      "Epoch: 3/10 Iteration: 152 Train loss: 0.175\n",
      "Epoch: 3/10 Iteration: 160 Train loss: 0.182\n",
      "Epoch: 4/10 Iteration: 168 Train loss: 0.177\n",
      "Epoch: 4/10 Iteration: 176 Train loss: 0.174\n",
      "Epoch: 4/10 Iteration: 184 Train loss: 0.179\n",
      "Epoch: 4/10 Iteration: 192 Train loss: 0.167\n",
      "Epoch: 4/10 Iteration: 200 Train loss: 0.171\n",
      "Val acc: 0.758\n",
      "Epoch: 5/10 Iteration: 208 Train loss: 0.168\n",
      "Epoch: 5/10 Iteration: 216 Train loss: 0.163\n",
      "Epoch: 5/10 Iteration: 224 Train loss: 0.158\n",
      "Epoch: 5/10 Iteration: 232 Train loss: 0.149\n",
      "Epoch: 5/10 Iteration: 240 Train loss: 0.151\n",
      "Epoch: 6/10 Iteration: 248 Train loss: 0.159\n",
      "Val acc: 0.775\n",
      "Epoch: 6/10 Iteration: 256 Train loss: 0.149\n",
      "Epoch: 6/10 Iteration: 264 Train loss: 0.158\n",
      "Epoch: 6/10 Iteration: 272 Train loss: 0.151\n",
      "Epoch: 6/10 Iteration: 280 Train loss: 0.157\n",
      "Epoch: 7/10 Iteration: 288 Train loss: 0.148\n",
      "Epoch: 7/10 Iteration: 296 Train loss: 0.148\n",
      "Val acc: 0.786\n",
      "Epoch: 7/10 Iteration: 304 Train loss: 0.142\n",
      "Epoch: 7/10 Iteration: 312 Train loss: 0.141\n",
      "Epoch: 7/10 Iteration: 320 Train loss: 0.144\n",
      "Epoch: 8/10 Iteration: 328 Train loss: 0.138\n",
      "Epoch: 8/10 Iteration: 336 Train loss: 0.151\n",
      "Epoch: 8/10 Iteration: 344 Train loss: 0.142\n",
      "Val acc: 0.756\n",
      "Epoch: 8/10 Iteration: 352 Train loss: 0.132\n",
      "Epoch: 8/10 Iteration: 360 Train loss: 0.143\n",
      "Epoch: 9/10 Iteration: 368 Train loss: 0.137\n",
      "Epoch: 9/10 Iteration: 376 Train loss: 0.133\n",
      "Epoch: 9/10 Iteration: 384 Train loss: 0.132\n",
      "Epoch: 9/10 Iteration: 392 Train loss: 0.139\n",
      "Epoch: 9/10 Iteration: 400 Train loss: 0.131\n",
      "Val acc: 0.769\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(X_train, y_train, batchSize), 1):\n",
    "            feed = {inputs: x,\n",
    "                    labels: y[:, None],\n",
    "                    keep_prob: 0.5}\n",
    "            loss, state = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%8==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%50==0:\n",
    "                val_acc = []\n",
    "                for x, y in get_batches(X_test, y_test, batchSize):\n",
    "                    feed = {inputs: x,\n",
    "                            labels: y[:, None],\n",
    "                            keep_prob: 1}\n",
    "                    batch_acc = sess.run([accuracy], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El porcentaje de precision que se obtuvo con el modelo implementado es de 76,9%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
