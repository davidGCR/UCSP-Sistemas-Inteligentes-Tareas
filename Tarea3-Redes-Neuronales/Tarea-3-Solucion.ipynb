{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/outofcore_modelpersistence.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDb Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train a simple logistic regression model to classify movie reviews from the 50k IMDb review dataset that has been collected by Maas et. al.\n",
    "\n",
    "> AL Maas, RE Daly, PT Pham, D Huang, AY Ng, and C Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics\n",
    "\n",
    "[Source: http://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "The dataset consists of 50,000 movie reviews from the original \"train\" and \"test\" subdirectories. The class labels are binary (1=positive and 0=negative) and contain 25,000 positive and 25,000 negative movie reviews, respectively.\n",
    "For simplicity, I assembled the reviews in a single CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# if you want to download the original file:\n",
    "#df = pd.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/50k_imdb_movie_reviews.csv')\n",
    "# otherwise load local file\n",
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us shuffle the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## uncomment these lines if you have dowloaded the original file:\n",
    "#np.random.seed(0)\n",
    "#df = df.reindex(np.random.permutation(df.index))\n",
    "#df[['review', 'sentiment']].to_csv('shuffled_movie_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define a simple `tokenizer` that splits the text into individual word tokens. Furthermore, we will use some simple regular expression to remove HTML markup and all non-letter characters but \"emoticons,\" convert the text to lower case, remove stopwords, and apply the Porter stemming algorithm to convert the words into their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it at try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', ':)', ':)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('This :) is a <a> test! :-)</br>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning (SciKit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a generator that returns the document body and the corresponding class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conform that the `stream_docs` function fetches the documents as intended, let us execute the following code snippet before we implement the `get_minibatch` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='shuffled_movie_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we confirmed that our `stream_docs` functions works, we will now implement a `get_minibatch` function to fetch a specified number (`size`) of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        docs.append(text)\n",
    "        y.append(label)\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will make use of the \"hashing trick\" through scikit-learns [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) to create a bag-of-words model of our documents. Details of the bag-of-words model for document classification can be found at  [Naive Bayes and Text Classification I - Introduction and Theory](http://arxiv.org/abs/1410.5329)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "# Exercise 1: define features based on word embeddings (pre-trained word2vec / Glove/Fastext emebddings can be used)\n",
    "# Define suitable d dimension, and sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [SGDClassifier]() from scikit-learn, we will can instanciate a logistic regression classifier that learns from the documents incrementally using stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "doc_stream = stream_docs(path='shuffled_movie_data.csv')\n",
    "# Exercise 2: Define at least a Three layer neural network. Define its structure (number of hidden neurons, etc)\n",
    "# Define a nonlinear function for hidden layers.\n",
    "# Define a suitable loss function for binary classification\n",
    "# Implement the backpropagation algorithm for this structure\n",
    "# Do not use Keras / Tensorflow /PyTorch etc. libraries\n",
    "# Train the model using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#import pyprind\n",
    "#pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    X_train = vect.transform(X_train)\n",
    "#     print(y_train.shape)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    #pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your machine, it will take about 2-3 minutes to stream the documents and learn the weights for the logistic regression model to classify \"new\" movie reviews. Executing the preceding code, we used the first 45,000 movie reviews to train the classifier, which means that we have 5,000 reviews left for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))\n",
    "#Exercise 3: compare  with your Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that the predictive performance, an accuracy of ~87%, is quite \"reasonable\" given that we \"only\" used the default parameters and didn't do any hyperparameter optimization. \n",
    "\n",
    "After we estimated the model perfomance, let us use those last 5,000 test samples to update our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we successfully trained a model to predict the sentiment of a movie review. Unfortunately, if we'd close this IPython notebook at this point, we'd have to go through the whole learning process again and again if we'd want to make a prediction on \"new data.\"\n",
    "\n",
    "So, to reuse this model, we could use the [`pickle`](https://docs.python.org/3.5/library/pickle.html) module to \"serialize a Python object structure\". Or even better, we could use the [`joblib`](https://pypi.python.org/pypi/joblib) library, which handles large NumPy arrays more efficiently.\n",
    "\n",
    "To install:\n",
    "conda install -c anaconda joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./clf.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "if not os.path.exists('./pkl_objects'):\n",
    "    os.mkdir('./pkl_objects')\n",
    "    \n",
    "joblib.dump(vect, './vectorizer.pkl')\n",
    "joblib.dump(clf, './clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code above, we \"pickled\" the `HashingVectorizer` and the `SGDClassifier` so that we can re-use those objects later. However, `pickle` and `joblib` have a known issue with `pickling` objects or functions from a `__main__` block and we'd get an `AttributeError: Can't get attribute [x] on <module '__main__'>` if we'd unpickle it later. Thus, to pickle the `tokenizer` function, we can write it to a file and import it to get the `namespace` \"right\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tokenizer.py\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tokenizer.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizer import tokenizer\n",
    "joblib.dump(tokenizer, './tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us restart this IPython notebook and check if the we can load our serialized objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "tokenizer = joblib.load('./tokenizer.pkl')\n",
    "vect = joblib.load('./vectorizer.pkl')\n",
    "clf = joblib.load('./clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the `tokenizer`, `HashingVectorizer`, and the tranined logistic regression model, we can use it to make predictions on new data, which can be useful, for example, if we'd want to embed our classifier into a web application -- a topic for another IPython notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ['I did not like this movie']\n",
    "X = vect.transform(example)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ['I loved this movie']\n",
    "X = vect.transform(example)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solucion de Tareas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: WordEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio se utiliza word Embeddigs preentrenados Glove. Primeramente se carga el dataset preentrenado, a continuacion se define un vocabulario del data set original (movie reviews) y finalmente se convierten los documentos a vectores numericos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAW DATA\n",
    "doc_stream = stream_docs(path='shuffled_movie_data.csv')\n",
    "TOTAL_EXAMPLES = 50000\n",
    "\n",
    "X_full , y_full = get_minibatch(doc_stream, size=TOTAL_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definiendo nuevas caracteristicas usando Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuracion\n",
    "MAX_NB_WORDS = 1000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "N_FEATURES = 50 #dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga de word Embedings preentrenados\n",
    "glove_file = 'pretrained/glove.twitter.27B.' + str(N_FEATURES) + 'd.txt'\n",
    "emb_dict = {}\n",
    "glove = open(glove_file)\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    emb_dict[word] = vector\n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124252 palabras unicas.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Creacion del vocabulario del dataset\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#MAX_NB_WORDS = 1000\n",
    "#MAX_SEQUENCE_LENGTH = 1000\n",
    "#N_FEATURES = 500 #dimensiones\n",
    "\n",
    "tokenizer_k = Tokenizer(nb_words=MAX_NB_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,split=\" \")\n",
    "tokenizer_k.fit_on_texts(X_full)\n",
    "sequences = tokenizer_k.texts_to_sequences(X_full)\n",
    "word_index = tokenizer_k.word_index #vocabulario del dataset\n",
    "N_WORDS = len(word_index) #numero de palabras unicas en el dataset\n",
    "print('%s palabras unicas.' %N_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizacion del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que convierte un documento en un vector promediando vectores \"wordEmbeddings\" \n",
    "# en un documento\n",
    "def document2Vector(vocabulario_emb,n_features,documento):\n",
    "    # n_features: dimension del vector\n",
    "    # documentos: documento (string)\n",
    "    vector = np.zeros((1,n_features),dtype=\"float64\")\n",
    "    words = tokenizer(documento)\n",
    "    nwords = 0\n",
    "    for word in words:\n",
    "        #print(word)\n",
    "        word_emb = vocabulario_emb.get(word)\n",
    "        if word_emb is not None:\n",
    "            vector = vector+word_emb\n",
    "            nwords = nwords + 1.\n",
    "    if nwords > 0:\n",
    "        vector = vector/nwords\n",
    "    vector = vector.reshape((1,n_features))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion que transforma documentos(corpus) a vectores numericos usando word embeddings.\n",
    "def corpus2Matrix(corpus,n_examples, n_features):\n",
    "    matrix = np.zeros((n_examples, n_features))\n",
    "    for i in range(n_examples):\n",
    "        vector = document2Vector(emb_dict,n_features,corpus[i])\n",
    "        matrix[i,:]=vector\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicion de nuevas Caracteristicas\n",
    "Se procede a mapear los word Embedding a los documentos. Como los documentos son un conjunto de palabras, se procedio a calcular el promedio de los word Embeddings que componen un documento para obtener un vector que represente al documento(movie review). Este proceso puede demorar de 4 a 5 minutos por lo que se realizo una sola vez y se guardo el resultado en un csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINICION DE NUEVAS CARACTERISTICAS: Llamado de las funciones antes definidas.\n",
    "#Este proceso puede demorar, por lo que solo se ejecuto una vez y se guardo la matriz en\n",
    "#un csv, si desea ejecutar descomente las siguientes 2 lineas\n",
    "#mm=corpus2Matrix(X_full,TOTAL_EXAMPLES, N_FEATURES)\n",
    "#np.savetxt('matrix_embeding.csv', mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "X=np.loadtxt('matrix_embeding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.32962105e+02,  2.10725443e+02,  1.55017363e+01,  8.40763092e+01,\n",
       "        7.92159411e+01,  2.86586536e+02,  9.04910358e+02, -2.36633981e+02,\n",
       "       -1.82299615e+02,  1.61963325e+02, -4.05243644e+01, -1.24028262e+02,\n",
       "       -3.84192170e+03, -2.06049839e+01, -2.16941698e+02,  2.11767037e+01,\n",
       "        2.13507740e+02, -1.96449158e+02, -1.55626545e+02, -8.81555390e+01,\n",
       "        7.58261087e+01, -3.86509834e+01, -7.65415855e+01,  1.90189775e+02,\n",
       "       -1.42897729e+02,  6.17523400e+02, -1.26982207e+02,  8.89971795e+01,\n",
       "       -1.81705949e+02, -1.76245184e+02, -7.33593443e-01, -9.08780513e+01,\n",
       "       -5.60791226e+01,  3.49233080e+01,  2.46382283e+02,  9.17318119e+01,\n",
       "       -1.31053947e+02, -1.84546897e+02,  2.04173912e+02, -1.01386669e+02,\n",
       "       -3.25451896e+02,  2.00178957e+02,  2.67673428e+02, -3.65778283e+01,\n",
       "        9.00153811e+01, -1.20048910e+02,  2.05044827e+02,  2.73416869e+02,\n",
       "       -2.22464404e+02,  1.64775535e+01])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[4999,:]*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2: Implementacion de Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division de datos de entrenamiento, validacion y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "m_train = 45000 #numero de ejemplos de entrenamiento\n",
    "m_test = 5000 #numero de ejemplos de prueba\n",
    "y_full = np.array(y_full)\n",
    "y_full = y_full.reshape(TOTAL_EXAMPLES,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_full, test_size=0.1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo usando sklearn\n",
    "En esta parte se entreno un modelo de SKlearn con las nuevas caracteristicas definidas, la sxactitud que se obtuvo coneste modelo es de 78.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/entorno1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=None,\n",
       "       n_iter=15, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=1, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(alpha=0.001,loss='log', random_state=1, n_iter=15)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.781\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ejemplos entrenamiento:  (50, 36000)\n",
      "ejemplos entrenamiento (labels):  (1, 36000)\n",
      "ejemplos validacion:  (50, 9000)\n",
      "ejemplos validacion (labels):  (1, 9000)\n",
      "ejemplos test:  (50, 5000)\n",
      "ejemplos test (labels):  (1, 5000)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.transpose(X_train)\n",
    "y_train = np.transpose(y_train)\n",
    "X_test = np.transpose(X_test)\n",
    "y_test = np.transpose(y_test)\n",
    "X_val = np.transpose(X_val)\n",
    "y_val = np.transpose(y_val)\n",
    "print('ejemplos entrenamiento: ',X_train.shape)\n",
    "print('ejemplos entrenamiento (labels): ',y_train.shape)\n",
    "print('ejemplos validacion: ',X_val.shape)\n",
    "print('ejemplos validacion (labels): ',y_val.shape)\n",
    "print('ejemplos test: ',X_test.shape)\n",
    "print('ejemplos test (labels): ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicion de la Arquitectura de la Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las siguientes secciones se definen funciones para una red neuronal de 3 capas. La primera capa consta de 50 neuronas (Caracteristicas Word Embeddings), la capa oculta con 4 neuronas (con la tangente hyperbolica como funcion de activacion) y la capa de salida con una neurona cuya funcion de activacion es la sigmoide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que define la arquitectura de la red neuronal\n",
    "def layer_sizes(X, Y, n_h):\n",
    "    \"\"\"\n",
    "    X -- ejemplos de entrenamiento\n",
    "    Y -- etiquetas de X\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- numero de neuronas en la capa de entrada\n",
    "    n_h -- numero de neuronas de la capa oculta\n",
    "    n_y -- numero de neuronas de la capa de salida\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0] # input layer\n",
    "    n_h = n_h\n",
    "    n_y = Y.shape[0] # output layer\n",
    "    \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de capa de entrada: n_x = 50\n",
      "Tamaño de capa  oculta: n_h = 4\n",
      "Tamaño de capa de salida: n_y = 1\n"
     ]
    }
   ],
   "source": [
    "(n_x, n_h, n_y) = layer_sizes(X_train, y_train, 4)\n",
    "print(\"Tamaño de capa de entrada: n_x = \" + str(n_x))\n",
    "print(\"Tamaño de capa  oculta: n_h = \" + str(n_h))\n",
    "print(\"Tamaño de capa de salida: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializacion de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    n_x -- Tamaño de capa de entrada\n",
    "    n_h -- Tamaño de capa  oculta\n",
    "    n_y -- Tamaño de capa de salida\n",
    "    \n",
    "    Returns:\n",
    "    params -- diccionario con parametros\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = (4, 50)\n",
      "b1 = (4, 1)\n",
      "W2 = (1, 4)\n",
      "b2 = (1, 1)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"].shape))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"].shape))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"].shape))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de activacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definicion de funcion sigmoide\n",
    "def sigmoid(z):\n",
    "    s=1/(1+np.exp(-z))    \n",
    "    return s\n",
    "\n",
    "#Funcion tangente hyperbolica\n",
    "def s_tanh(z):\n",
    "    t = np.tanh(z)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    X -- datos de entrenamiento(n_x, m)\n",
    "    parameters -- diccionario con parametros inicializados\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- Activacion de la capa final (sigmoide)\n",
    "    cache -- Diccionario con calculos intermedios \"Z1\", \"A1\", \"Z2\" y \"A2\"\n",
    "    \"\"\"\n",
    "    # Parametros\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "#     A1 = sigmoid(Z1)\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_compute_cost(a2, yi, parameters,lambd,m):\n",
    "    \"\"\"\n",
    "    a2 -- Salida de activacion de la ultima capa para un ejemplo\n",
    "    yi -- etiqueta del ejemplo\n",
    "    parameters -- diccionario con parametros W1, b1, W2 and b2\n",
    "    lamb -- parametro de regularizacion\n",
    "    m -- numero de ejemplos de entrenamiento\n",
    "    \n",
    "    Returns:\n",
    "    cost -- funcion de costo (cross-entropy)\n",
    "    \"\"\"\n",
    "    \n",
    "    a2 = np.squeeze(a2)\n",
    "    yi = np.squeeze(yi)\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    L2_regularizacion = (lambd/(2*m))*(np.sum(np.square(W2))+np.sum(np.square(W1)))\n",
    "    \n",
    "    logprobs = yi*np.log(a2) + (1-yi)*np.log(1-a2)\n",
    "    cost = -logprobs + L2_regularizacion\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "#     print('costo: ',cost)\n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Funcion que se encarga de actualizar los parametros despues de una epoca\n",
    "\n",
    "def s_update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    parameters -- parametros\n",
    "    grads -- gradientes\n",
    "    learning_rate -- coeficiente de aprendizaje\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parametros actualizados\n",
    "    \"\"\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # Actualizacion\n",
    "    W1 = W1-learning_rate*dW1\n",
    "    b1 = b1-learning_rate*db1\n",
    "    W2 = W2-learning_rate*dW2\n",
    "    b2 = b2-learning_rate*db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_backward_propagation(parameters, cache, X, Y, i,lambd):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Activaciones de forward prop.\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "   \n",
    "    #indice aleatorio\n",
    "#     i = np.random.random_integers(0, m-1) \n",
    "#     print('i',i)\n",
    "   \n",
    "    \n",
    "    grads = {\"dW1\": None,\n",
    "             \"db1\": None,\n",
    "             \"dW2\": None,\n",
    "             \"db2\": None}\n",
    "      \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "        \n",
    "    a1 = A1[:,i].reshape(A1[:,i].shape[0],1) #activacion de un ejemplo en la primera capa\n",
    "    a2 = A2[:,i].reshape(A2[:,i].shape[0],1) #activacion de un ejemplo en la ultima capa\n",
    "\n",
    "    xi = X[:,i].reshape(X[:,i].shape[0],1)#ejemplo aleatorio\n",
    "    yi = Y[:,i].reshape(Y[:,i].shape[0],1)#etiqueta\n",
    "\n",
    "        ##Calculo del costo para un unico ejemplo\n",
    "    costo = s_compute_cost(a2, yi, parameters,lambd,m)\n",
    "#         costo = compute_cost(A2, Y, parameters)\n",
    "        \n",
    "        ## Backward propagation: calculo de dW1, db1, dW2, db2. \n",
    "    dZ2 = a2 - yi    \n",
    "    grads[\"dW2\"] = dZ2*np.transpose(a1) + (lambd/m)*(W2)\n",
    "    grads[\"db2\"] = dZ2#1x1\n",
    "#     gp = sigmoid(a1)*(1-sigmoid(a1))#derivada de sigmoide\n",
    "    gp = 1-np.power(a1,2)#derivada de tanh\n",
    "    dZ1 = np.multiply(np.dot(np.transpose(W2),dZ2),gp)\n",
    "    grads[\"dW1\"] = np.dot(dZ1,np.transpose(xi))+ (lambd/m)*(W1)\n",
    "    grads[\"db1\"] = dZ1\n",
    "        ##update\n",
    "    \n",
    "#     grads = {\"dW1\": dW1,\n",
    "#              \"db1\": db1,\n",
    "#              \"dW2\": dW2,\n",
    "#              \"db2\": db2}\n",
    "    \n",
    "    return  costo, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente Descendente Estocastico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_modelo(X, Y, n_h, num_iterations, print_cost,learning_rate,lambd):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    X -- conjunto de entrenamiento\n",
    "    Y -- etiquetas de entrenamiento\n",
    "    n_h -- numero de neuronas en la capa oculta\n",
    "    num_iterations -- Numero de epocas \n",
    "    print_cost -- imprimir costos\n",
    "    \n",
    "    Retorna:\n",
    "    parameters -- parametros aprendidos.\n",
    "    costos -- array historico de costos\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x, n_h, n_y = layer_sizes(X, Y,n_h)\n",
    "    print('(n_x, n_h, n_y):',n_x, n_h, n_y)\n",
    "#     n_h = layer_sizes(X, Y)[1]\n",
    "#     n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "   ##Inicializacion de parametros\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    ph_W1 = []\n",
    "    ph_b1 = []\n",
    "    ph_W2 = []\n",
    "    ph_b2 = []\n",
    "    \n",
    "    # Gradiente Descendente Estocatico\n",
    "    costos = []\n",
    "    m = X.shape[1]\n",
    "        \n",
    "    for j in range(0, num_iterations):\n",
    "#         i = np.random.random_integers(0, m-1)\n",
    "#         # Forward propagation.\n",
    "#         A2, cache = s_forward_propagation(X, parameters)\n",
    "#         print('iteracion: ',j)\n",
    "#         # Backpropagation. \n",
    "#         costo, grads = s_backward_propagation(parameters, cache, X, Y,i)\n",
    "#         costos.append(costo)\n",
    "#          # Gradient descent parameter update. \n",
    "#         parameters = s_update_parameters(parameters, grads,learning_rate)\n",
    "#         if(costo<0.01):\n",
    "#             break\n",
    "        for i in range(0,m):\n",
    "            ph_W1.append(parameters[\"W1\"])\n",
    "            ph_b1.append(parameters[\"b1\"])\n",
    "            ph_W2.append(parameters[\"W2\"])\n",
    "            ph_b2.append(parameters[\"b2\"])\n",
    "            \n",
    "            # Forward propagation.\n",
    "            A2, cache = s_forward_propagation(X, parameters)\n",
    "#             print('iteracion: ',i)\n",
    "            # Backpropagation. \n",
    "            costo, grads = s_backward_propagation(parameters, cache, X, Y,i,lambd)\n",
    "            costos.append(costo)\n",
    "            # Gradient descent parameter update. \n",
    "            \n",
    "            parameters = s_update_parameters(parameters, grads,learning_rate)\n",
    "            if(costo<0.01):\n",
    "                break\n",
    "    \n",
    "   \n",
    "    parameters_h = {\"hW1\": ph_W1,\n",
    "             \"hb1\": ph_b1,\n",
    "             \"hW2\": ph_W2,\n",
    "             \"hb2\": ph_b2}\n",
    "    \n",
    "    parameters = calculate_mean_parameters(parameters,parameters_h)\n",
    "    \n",
    "   \n",
    "    return costos,parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que el algoritmo SGD calcula costos que oscilan entre el minimo global, se promedia los parametros para una mejor aproximacion. (Fuente: https://www.coursera.org/learn/ml-classification/lecture/iDOIY/don-t-trust-last-coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion que calcula la media de todos los parametros calculados\n",
    "\n",
    "def calculate_mean_parameters(parameters, parameters_history):\n",
    "    ph_W1 = parameters_history[\"hW1\"]\n",
    "    ph_b1 = parameters_history[\"hb1\"]\n",
    "    ph_W2 = parameters_history[\"hW2\"]\n",
    "    ph_b2 = parameters_history[\"hb2\"]\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    length = len(ph_W1)\n",
    "    for k in range(0,length-1):\n",
    "        W1 = W1 + ph_W1[k]\n",
    "        b1 = b1 + ph_b1[k]\n",
    "        W2 = W2 + ph_W2[k]\n",
    "        b2 = b2 + ph_b2[k]\n",
    "        \n",
    "    W1 = (1/length)*W1\n",
    "    b1 = (1/length)*b1\n",
    "    W2 = (1/length)*W2\n",
    "    b2 = (1/length)*b2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funcion que realiza predicciones\n",
    "## Threshold = 0.5\n",
    "def s_predict(parameters, X):\n",
    "    A2, cache = s_forward_propagation(X,parameters)\n",
    "    predictions = (A2 > 0.5).astype(int)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Funcion para graficar la convergencia de un algoritmo\n",
    "import matplotlib.pyplot as plt\n",
    "def plotConvergence(jvec,iterations):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(len(jvec)),jvec,'bo')\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Convergencia Stochastic Gradient Descendt\")\n",
    "    plt.xlabel(\"Numero de Iteraciones\")\n",
    "    plt.ylabel(\"Funcion de Costo\")\n",
    "    dummy = plt.xlim([-0.05*iterations,1.05*iterations])\n",
    "#     dummy = plt.xlim([-0.05*iterations,10.0*iterations])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_x, n_h, n_y): 50 4 1\n"
     ]
    }
   ],
   "source": [
    "costos, parameters = nn_modelo(X_train, y_train, 4, num_iterations=1, \n",
    "                               print_cost=True,learning_rate=0.01, lambd = 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# costos\n",
    "# len(costos)\n",
    "# plotConvergence(costos,len(costos))\n",
    "# length = 500\n",
    "# plotConvergence(costos[len(costos)-length:len(costos)],length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud:  77.58\n"
     ]
    }
   ],
   "source": [
    "predictions = s_predict(parameters, X_test)\n",
    "accuracy = float((np.dot(y_test,predictions.T) + np.dot(1-y_test,1-predictions.T))/float(y_test.size)*100)\n",
    "print('Exactitud: ',accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7758"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predictions == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio3: Comparacion de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando con el modelo de SKlearn con las nuevas caracteristicas definidas, la exactitud que se obtuvo con la red neuronal es de 77.6% mientras que con SKLearn es de 78.1%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
